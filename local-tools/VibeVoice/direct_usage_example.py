#!/usr/bin/env python3
"""
VibeVoice ç›´æ¥è°ƒç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ç›´æ¥ä½¿ç”¨ Python æ¨¡å—è¿›è¡Œæ–‡æœ¬è½¬è¯­éŸ³
"""

import argparse
import torch
import numpy as np
import soundfile as sf
import librosa
from pathlib import Path

from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference
from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
from transformers import set_seed


def load_voice_sample(voice_path: str, target_sr: int = 24000) -> np.ndarray:
    """åŠ è½½è¯­éŸ³æ ·æœ¬æ–‡ä»¶"""
    try:
        audio, sr = sf.read(voice_path)
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)  # è½¬ä¸ºå•å£°é“
        if sr != target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
        return audio.astype(np.float32)
    except Exception as e:
        raise ValueError(f"Failed to load voice sample from {voice_path}: {e}")


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="VibeVoice Direct Usage Example")
    parser.add_argument("--text", type=str, required=True, help="Text to synthesize")
    parser.add_argument("--voice_samples", type=str, nargs="+", required=True, 
                       help="Path to voice sample files (one for each speaker)")
    parser.add_argument("--speakers", type=str, nargs="+", default=["Alice", "Bob"],
                       help="Speaker names (default: Alice Bob)")
    parser.add_argument("--model_path", type=str, default="microsoft/VibeVoice-1.5B",
                       help="Path to VibeVoice model")
    parser.add_argument("--device", type=str, 
                       default="cuda" if torch.cuda.is_available() else "cpu",
                       help="Device for inference")
    parser.add_argument("--output", type=str, default="output_example.wav",
                       help="Output audio file path")
    parser.add_argument("--cfg_scale", type=float, default=1.3,
                       help="CFG scale for generation")
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if len(args.voice_samples) != len(args.speakers):
        raise ValueError(f"Number of voice samples ({len(args.voice_samples)}) must match number of speakers ({len(args.speakers)})")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    print(f"ğŸ™ï¸ VibeVoice Direct Usage Example")
    print(f"ğŸ“± Device: {args.device}")
    print(f"ğŸ“¦ Model: {args.model_path}")
    print(f"ğŸ­ Speakers: {', '.join(args.speakers)}")
    print(f"ğŸµ Voice samples: {', '.join(args.voice_samples)}")
    print()
    
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    print("ğŸ”„ Loading model...")
    processor = VibeVoiceProcessor.from_pretrained(args.model_path)
    
    model = VibeVoiceForConditionalGenerationInference.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16 if args.device == "cuda" else torch.float32,
        device_map=args.device if args.device != "cpu" else None,
        attn_implementation="sdpa",  # é¿å… FlashAttention2 ä¾èµ–
    )
    
    if args.device == "cpu":
        model = model.to("cpu")
    
    model.eval()
    model.set_ddpm_inference_steps(num_steps=5)
    
    print("âœ… Model loaded successfully")
    print()
    
    # å‡†å¤‡æ–‡æœ¬
    print(f"ğŸ“ Text to synthesize:")
    print(args.text)
    print()
    
    # åŠ è½½è¯­éŸ³æ ·æœ¬
    print("ğŸµ Loading voice samples...")
    voice_samples = []
    for i, voice_path in enumerate(args.voice_samples):
        print(f"  Loading {args.speakers[i]}: {voice_path}")
        try:
            sample = load_voice_sample(voice_path)
            voice_samples.append(sample)
            print(f"    âœ… Loaded: {sample.shape} samples")
        except Exception as e:
            print(f"    âŒ Failed: {e}")
            return 1
    
    # å¤„ç†è¾“å…¥
    print("ğŸ”§ Processing inputs...")
    inputs = processor(
        text=[args.text],
        voice_samples=[voice_samples],
        padding=True,
        return_tensors="pt",
        return_attention_mask=True,
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    if args.device != "cpu":
        for k, v in inputs.items():
            if torch.is_tensor(v):
                inputs[k] = v.to(args.device)
    
    # ç”ŸæˆéŸ³é¢‘
    print("ğŸ™ï¸ Generating speech...")
    import time
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=None,
            cfg_scale=args.cfg_scale,
            tokenizer=processor.tokenizer,
            generation_config={'do_sample': False},
            verbose=False,
        )
    
    generation_time = time.time() - start_time
    
    # å¤„ç†è¾“å‡º
    # VibeVoice è¾“å‡ºé€šå¸¸æ˜¯ VibeVoiceGenerationOutput å¯¹è±¡
    print(f"ğŸ” Output type: {type(outputs)}")
    print(f"ğŸ” Available attributes: {[attr for attr in dir(outputs) if not attr.startswith('_')]}")
    
    if hasattr(outputs, 'speech_outputs'):
        # å…³é”®ï¼éŸ³é¢‘æ•°æ®åœ¨ speech_outputs ä¸­
        audio_tensor = outputs.speech_outputs[0]  # å–ç¬¬ä¸€ä¸ªbatch
        print("ğŸ“¦ Using outputs.speech_outputs[0] (CORRECT)")
        print(f"ğŸ“Š Audio tensor shape: {audio_tensor.shape}")
    elif hasattr(outputs, 'audio'):
        # å¦‚æœè¾“å‡ºæœ‰ audio å±æ€§
        audio_tensor = outputs.audio
        print("ğŸ“¦ Using outputs.audio")
    elif hasattr(outputs, 'sequences'):
        # è¿™ä¸ªæ˜¯æ–‡æœ¬tokensï¼Œä¸æ˜¯éŸ³é¢‘ï¼
        print("âš ï¸  Warning: sequences contains text tokens, not audio!")
        audio_tensor = outputs.sequences
        print("ğŸ“¦ Using outputs.sequences (may be wrong)")
    elif torch.is_tensor(outputs):
        # å¦‚æœç›´æ¥æ˜¯tensor
        audio_tensor = outputs
        print("ğŸ“¦ Using outputs directly (tensor)")
    else:
        # å°è¯•è·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        if hasattr(outputs, '__getitem__'):
            audio_tensor = outputs[0]
            print("ğŸ“¦ Using outputs[0]")
        else:
            raise ValueError(f"Unsupported output type: {type(outputs)}")
    
    # è½¬æ¢tensorä¸ºnumpy
    if torch.is_tensor(audio_tensor):
        if audio_tensor.dtype == torch.bfloat16:
            audio_tensor = audio_tensor.float()
        audio_data = audio_tensor.cpu().numpy().astype(np.float32)
        print(f"ğŸ“Š Final audio data shape: {audio_data.shape}")
    else:
        audio_data = np.array(audio_tensor, dtype=np.float32)
        print(f"ğŸ“Š Audio data type: {type(audio_tensor)}")
    
    # ç¡®ä¿æ˜¯1Dæ•°ç»„
    if len(audio_data.shape) > 1:
        audio_data = audio_data.squeeze()
    
    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    output_path = Path(args.output)
    sf.write(output_path, audio_data, 24000)
    
    duration = len(audio_data) / 24000
    
    print(f"âœ… Generation completed!")
    print(f"â±ï¸ Generation time: {generation_time:.2f} seconds")
    print(f"ğŸµ Audio duration: {duration:.2f} seconds")
    print(f"ğŸ’¾ Output saved to: {output_path.absolute()}")
    print()
    print("ğŸ§ You can now play the generated audio file!")
    
    return 0


if __name__ == "__main__":
    exit(main())